from __future__ import annotations

import json
import logging
import os
import uuid
import zipfile
from dataclasses import dataclass
from io import BytesIO
from typing import Dict, List, Optional, Tuple
from datetime import datetime

import mercadopago
import pandas as pd
import numpy as np
import requests
from flask import (
    Flask,
    jsonify,
    redirect,
    render_template,
    request,
    send_file,
    url_for,
    session,
)
from flask_cors import CORS
from openai import OpenAI
import matplotlib.pyplot as plt
import seaborn as sns
from reportlab.lib.pagesizes import letter, A4
from reportlab.pdfgen import canvas
from reportlab.lib.utils import ImageReader
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, Image as ReportLabImage
from reportlab.lib import colors
from reportlab.lib.units import inch
import io
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# ==============================
# CONFIGURACI√ìN PROFESIONAL
# ==============================

class Config:
    # Seguridad empresarial
    SECRET_KEY = os.getenv("SECRET_KEY", "inertiax_pro_max_2025_secure_key")
    MAX_CONTENT_LENGTH = int(os.getenv("MAX_CONTENT_LENGTH_MB", "50")) * 1024 * 1024
    SESSION_COOKIE_NAME = "inertiax_pro_session"
    SESSION_COOKIE_SECURE = True
    SESSION_COOKIE_HTTPONLY = True

    # Archivos profesionales
    UPLOAD_DIR = os.getenv("UPLOAD_DIR", "/tmp/inertiax_pro")
    os.makedirs(UPLOAD_DIR, exist_ok=True)
    ALLOWED_EXT = {".csv", ".xls", ".xlsx", ".xlsm"}

    # Integraciones enterprise
    DOMAIN_URL = os.getenv("DOMAIN_URL", "https://inertiax-calculadora-1.onrender.com")
    MP_ACCESS_TOKEN = os.getenv("MP_ACCESS_TOKEN")
    MP_PUBLIC_KEY = os.getenv("MP_PUBLIC_KEY")
    
    # AI Configuration - Modelos de √∫ltima generaci√≥n
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
    OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o")

    # Sistema de acceso premium
    GUEST_CODES = set(
        (os.getenv("GUEST_CODES") or "INERTIAXVIP2025,ENTRENADORPRO,INVEXORTEST,PREMIUM2025").split(",")
    )

# ==============================
# INICIALIZACI√ìN ENTERPRISE
# ==============================

app = Flask(__name__, static_folder="static", template_folder="templates")
app.config.from_object(Config)
app.secret_key = app.config["SECRET_KEY"]

# CORS para integraciones empresariales
CORS(app, resources={r"/*": {"origins": "*"}}, supports_credentials=True)

# Logging profesional
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(name)s | %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('/tmp/inertiax_pro.log')
    ]
)
log = logging.getLogger("inertiax_pro")

# Clientes enterprise
mp = mercadopago.SDK(app.config["MP_ACCESS_TOKEN"]) if app.config["MP_ACCESS_TOKEN"] else None
ai_client = OpenAI(api_key=app.config["OPENAI_API_KEY"]) if app.config["OPENAI_API_KEY"] else None

# ==============================
# MODELOS DE DATOS PROFESIONALES
# ==============================

@dataclass
class AthleteAnalysis:
    name: str
    metrics: Dict
    trends: Dict
    recommendations: List[str]
    performance_score: float

@dataclass
class ExerciseAnalysis:
    name: str
    biomechanics: Dict
    load_progression: Dict
    velocity_profile: Dict
    efficiency_metrics: Dict

@dataclass
class SessionAnalysis:
    date: str
    fatigue_index: float
    performance_quality: float
    technical_consistency: float
    workload: float

def _job_dir(job_id: str) -> str:
    d = os.path.join(app.config["UPLOAD_DIR"], job_id)
    os.makedirs(d, exist_ok=True)
    return d

def _job_meta_path(job_id: str) -> str:
    return os.path.join(_job_dir(job_id), "meta.json")

def _save_meta(job_id: str, meta: Dict) -> str:
    p = _job_meta_path(job_id)
    with open(p, "w", encoding="utf-8") as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)
    return p

def _load_meta(job_id: str) -> Dict:
    p = _job_meta_path(job_id)
    if not os.path.exists(p):
        return {}
    with open(p, "r", encoding="utf-8") as f:
        return json.load(f)

def _ensure_job() -> str:
    if "job_id" not in session:
        session["job_id"] = uuid.uuid4().hex
    return session["job_id"]

def _allowed_file(name: str) -> bool:
    ext = os.path.splitext(name)[1].lower()
    return ext in app.config["ALLOWED_EXT"]

# ==============================
# DETECCI√ìN AUTOM√ÅTICA DE FORMATOS
# ==============================

def detect_file_type(df: pd.DataFrame, filename: str, origin: str) -> Dict:
    """
    Detecta autom√°ticamente el tipo de archivo y formato espec√≠fico
    """
    detection_result = {
        "origin_app": origin,
        "file_type": "desconocido",
        "sub_type": "desconocido",
        "confidence": 0.0,
        "columns_found": list(df.columns),
        "analysis_required": True
    }
    
    try:
        columns_str = ' '.join(df.columns.astype(str)).lower()
        filename_lower = filename.lower()
        
        # ===========================================
        # DETECCI√ìN PARA ENCODER HORIZONTAL
        # ===========================================
        if origin == "encoder_horizontal":
            detection_result["origin_app"] = "encoder_horizontal"
            
            # Patrones espec√≠ficos del Encoder Horizontal
            horizontal_patterns = {
                'resumen': ['fecha', 'velocidad_max', 'aceleracion_max', 'aceleracion_media', 'tiempo_reaccion', 'velocidad/peso'],
                'velocidad': ['tiempo', 'velocidad'],
                'aceleracion': ['tiempo', 'aceleracion'],
                'distancia': ['tiempo', 'distancia'],
                'velocidad_vs_distancia': ['distancia', 'velocidad']
            }
            
            # Detecci√≥n por nombre de archivo
            if 'velocidad' in filename_lower and 'distancia' in filename_lower:
                detection_result.update({"file_type": "velocidad_vs_distancia", "confidence": 0.9})
            elif 'velocidad' in filename_lower:
                detection_result.update({"file_type": "velocidad", "confidence": 0.8})
            elif 'aceleracion' in filename_lower or 'aceleraci√≥n' in filename_lower:
                detection_result.update({"file_type": "aceleracion", "confidence": 0.8})
            elif 'distancia' in filename_lower:
                detection_result.update({"file_type": "distancia", "confidence": 0.8})
            elif 'resumen' in filename_lower or 'sprint_data' in filename_lower:
                detection_result.update({"file_type": "resumen", "confidence": 0.9})
            else:
                # Detecci√≥n por columnas
                for file_type, pattern_list in horizontal_patterns.items():
                    matches = sum(1 for pattern in pattern_list if pattern in columns_str)
                    if matches >= 2:
                        detection_result.update({"file_type": file_type, "confidence": matches/len(pattern_list)})
                        break
            
            # Detecci√≥n por estructura
            if detection_result["file_type"] == "desconocido":
                if len(df.columns) >= 6 and any('fecha' in col.lower() for col in df.columns):
                    detection_result.update({"file_type": "resumen", "confidence": 0.7})
                elif len(df.columns) == 2:
                    col1, col2 = df.columns[0].lower(), df.columns[1].lower()
                    if 'tiempo' in col1 and 'velocidad' in col2:
                        detection_result.update({"file_type": "velocidad", "confidence": 0.8})
                    elif 'tiempo' in col1 and 'aceleracion' in col2:
                        detection_result.update({"file_type": "aceleracion", "confidence": 0.8})
                    elif 'tiempo' in col1 and 'distancia' in col2:
                        detection_result.update({"file_type": "distancia", "confidence": 0.8})
                    elif 'distancia' in col1 and 'velocidad' in col2:
                        detection_result.update({"file_type": "velocidad_vs_distancia", "confidence": 0.8})

        # ===========================================
        # DETECCI√ìN PARA ENCODER VERTICAL V1
        # ===========================================
        elif origin == "encoder_vertical_v1":
            detection_result["origin_app"] = "encoder_vertical_v1"
            
            # Patrones espec√≠ficos del Encoder Vertical V1 (PyQt6)
            v1_patterns = ['user', 'exercise', 'type', 'rep_number', 'load', 'max_velocity', 'avg_velocity', 'duration']
            matches = sum(1 for pattern in v1_patterns if pattern in columns_str)
            
            if matches >= 5:
                detection_result.update({
                    "file_type": "encoder_vertical_v1_completo", 
                    "sub_type": "fuerza_velocidad",
                    "confidence": matches/len(v1_patterns)
                })
            elif 'max_velocity' in columns_str and 'avg_velocity' in columns_str:
                detection_result.update({
                    "file_type": "encoder_vertical_v1_resumen", 
                    "sub_type": "metricas_velocidad",
                    "confidence": 0.7
                })
            else:
                detection_result.update({
                    "file_type": "encoder_vertical_v1_generico", 
                    "sub_type": "datos_genericos",
                    "confidence": 0.5
                })

        # ===========================================
        # DETECCI√ìN PARA ENCODER VERTICAL ANDROID
        # ===========================================
        elif origin == "app_android_encoder_vertical":
            detection_result["origin_app"] = "app_android_encoder_vertical"
            
            # Patrones del Encoder Vertical Android
            android_patterns = ['athlete', 'exercise', 'date', 'repetition', 'load(kg)', 'concentricvelocity(m/s)', 'eccentricvelocity(m/s)', 'maxvelocity(m/s)']
            matches = sum(1 for pattern in android_patterns if pattern in columns_str)
            
            if matches >= 4:
                detection_result.update({
                    "file_type": "encoder_vertical_android_completo",
                    "sub_type": "biomecanica_completa", 
                    "confidence": matches/len(android_patterns)
                })
            elif any('velocity' in col.lower() for col in df.columns):
                detection_result.update({
                    "file_type": "encoder_vertical_android_velocidad",
                    "sub_type": "analisis_velocidad",
                    "confidence": 0.6
                })
            else:
                detection_result.update({
                    "file_type": "encoder_vertical_android_generico",
                    "sub_type": "datos_entrenamiento", 
                    "confidence": 0.4
                })

        # Validar confianza m√≠nima
        if detection_result["confidence"] < 0.3:
            detection_result["file_type"] = "desconocido"
            detection_result["analysis_required"] = False

        log.info(f"üîç DETECCI√ìN: {detection_result}")

    except Exception as e:
        log.error(f"Error en detecci√≥n de formato: {str(e)}")
        detection_result["analysis_required"] = False

    return detection_result

# ==============================
# PROCESAMIENTO DE DATOS AVANZADO
# ==============================

def parse_dataframe(path: str) -> pd.DataFrame:
    """Procesamiento profesional de datos con m√∫ltiples validaciones"""
    try:
        ext = os.path.splitext(path)[1].lower()
        log.info(f"Procesando archivo: {path} (extensi√≥n: {ext})")
        
        if ext == ".csv":
            encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'windows-1252', 'cp1252']
            for encoding in encodings:
                try:
                    df = pd.read_csv(path, encoding=encoding)
                    log.info(f"Archivo CSV le√≠do con encoding: {encoding}")
                    return df
                except (UnicodeDecodeError, UnicodeError):
                    continue
            return pd.read_csv(path, encoding='utf-8', errors='replace')
        else:
            return pd.read_excel(path)
    except Exception as e:
        log.error(f"Error cr√≠tico procesando archivo: {str(e)}")
        raise

def preprocess_data_by_origin(df: pd.DataFrame, origin: str, file_type: str) -> pd.DataFrame:
    """
    Procesamiento cient√≠fico de datos seg√∫n el tipo de archivo detectado
    """
    log.info(f"Iniciando procesamiento cient√≠fico para: {origin} - {file_type}")
    
    # Standardizaci√≥n b√°sica de columnas
    df.columns = [str(col).strip().lower().replace(" ", "_").replace("(", "").replace(")", "") for col in df.columns]
    
    if origin == "encoder_horizontal":
        return preprocess_encoder_horizontal(df, file_type)
    elif origin == "encoder_vertical_v1":
        return preprocess_encoder_vertical_v1(df, file_type)
    elif origin == "app_android_encoder_vertical":
        return preprocess_encoder_vertical_android(df, file_type)
    else:
        return preprocess_generic_data(df)

def preprocess_encoder_horizontal(df: pd.DataFrame, file_type: str) -> pd.DataFrame:
    """Procesamiento para Encoder Horizontal"""
    try:
        # Procesamiento seg√∫n tipo espec√≠fico
        if file_type == "velocidad":
            if len(df.columns) >= 2:
                df.columns = ["tiempo_s", "velocidad_m_s"]
                df["velocidad_km_h"] = df["velocidad_m_s"] * 3.6
                
        elif file_type == "aceleracion":
            if len(df.columns) >= 2:
                df.columns = ["tiempo_s", "aceleracion_m_s2"]
                
        elif file_type == "distancia":
            if len(df.columns) >= 2:
                df.columns = ["tiempo_s", "distancia_m"]
                # Calcular velocidad instant√°nea
                if len(df) > 1:
                    df["velocidad_instantanea_m_s"] = df["distancia_m"].diff() / df["tiempo_s"].diff()
                    df["velocidad_instantanea_km_h"] = df["velocidad_instantanea_m_s"] * 3.6
                    
        elif file_type == "velocidad_vs_distancia":
            if len(df.columns) >= 2:
                df.columns = ["distancia_m", "velocidad_m_s"]
                df["velocidad_km_h"] = df["velocidad_m_s"] * 3.6
                
        elif file_type == "resumen":
            # Procesar archivo de resumen con m√∫ltiples m√©tricas
            column_mapping = {
                'fecha': 'fecha',
                'velocidad_max(m/s)': 'velocidad_maxima_m_s', 
                'aceleracion_max(m/s2)': 'aceleracion_maxima_m_s2',
                'aceleracion_media(m/s2)': 'aceleracion_media_m_s2',
                'tiempo_reaccion(s)': 'tiempo_reaccion_s',
                'velocidad/peso((m/s)/kg)': 'relacion_velocidad_peso'
            }
            
            for old_col, new_col in column_mapping.items():
                if old_col in df.columns:
                    df[new_col] = pd.to_numeric(df[old_col], errors='coerce')
                    
            if 'velocidad_maxima_m_s' in df.columns:
                df['velocidad_maxima_km_h'] = df['velocidad_maxima_m_s'] * 3.6

        # Conversi√≥n num√©rica para todas las columnas posibles
        for col in df.columns:
            if df[col].dtype == 'object':
                df[col] = pd.to_numeric(df[col], errors='ignore')

        log.info(f"Procesamiento Encoder Horizontal completado: {df.shape}")
        return df

    except Exception as e:
        log.error(f"Error en procesamiento Encoder Horizontal: {str(e)}")
        return df

def preprocess_encoder_vertical_v1(df: pd.DataFrame, file_type: str) -> pd.DataFrame:
    """Procesamiento para Encoder Vertical V1 (PyQt6)"""
    try:
        # Mapeo de columnas est√°ndar
        column_mapping = {
            'user': 'atleta',
            'exercise': 'ejercicio', 
            'type': 'tipo_ejercicio',
            'rep_number': 'repeticion',
            'load': 'carga_kg',
            'max_velocity': 'velocidad_maxima_m_s',
            'avg_velocity': 'velocidad_media_m_s', 
            'duration': 'duracion_s'
        }
        
        # Aplicar mapeo
        for old_col, new_col in column_mapping.items():
            if old_col in df.columns:
                df[new_col] = df[old_col]
        
        # Procesamiento num√©rico
        numeric_cols = ['carga_kg', 'velocidad_maxima_m_s', 'velocidad_media_m_s', 'duracion_s']
        for col in numeric_cols:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
        
        # C√°lculos avanzados para perfil fuerza-velocidad
        if 'carga_kg' in df.columns and 'velocidad_media_m_s' in df.columns:
            df['potencia_w'] = df['carga_kg'] * 9.81 * df['velocidad_media_m_s']  # P = F*v
            df['fuerza_n'] = df['carga_kg'] * 9.81
            
        # Identificar tipo de test
        if 'repeticion' in df.columns:
            if df['repeticion'].astype(str).str.contains('\.').any():
                df['tipo_test'] = 'perfil_fuerza_velocidad'
            else:
                df['tipo_test'] = 'entrenamiento_normal'

        log.info(f"Procesamiento Encoder Vertical V1 completado: {df.shape}")
        return df

    except Exception as e:
        log.error(f"Error en procesamiento Encoder Vertical V1: {str(e)}")
        return df

def preprocess_encoder_vertical_android(df: pd.DataFrame, file_type: str) -> pd.DataFrame:
    """Procesamiento para Encoder Vertical Android"""
    try:
        # Mapeo profesional de columnas
        rename_map = {
            "Athlete": "atleta",
            "Exercise": "ejercicio",
            "Date": "fecha",
            "Repetition": "repeticion",
            "Load(kg)": "carga_kg",
            "ConcentricVelocity(m/s)": "velocidad_concentrica_m_s",
            "EccentricVelocity(m/s)": "velocidad_eccentrica_m_s", 
            "MaxVelocity(m/s)": "velocidad_maxima_m_s",
            "Duration(s)": "duracion_s",
            "Estimated1RM": "estimado_1rm_kg",
            "Power(W)": "potencia_w",
            "Force(N)": "fuerza_n"
        }
        
        existing_rename_map = {k: v for k, v in rename_map.items() if k in df.columns}
        df.rename(columns=existing_rename_map, inplace=True)
        
        # Procesamiento num√©rico cient√≠fico
        numeric_columns = [
            "carga_kg", "velocidad_concentrica_m_s", "velocidad_eccentrica_m_s",
            "velocidad_maxima_m_s", "duracion_s", "estimado_1rm_kg", "potencia_w", "fuerza_n"
        ]
        
        for col in numeric_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors="coerce")
        
        # C√°lculos biomec√°nicos avanzados
        if "velocidad_concentrica_m_s" in df.columns and "carga_kg" in df.columns:
            df["potencia_relativa_w_kg"] = df["carga_kg"] * df["velocidad_concentrica_m_s"]
            df["impulso_mecanico"] = df["carga_kg"] * df["velocidad_concentrica_m_s"] * df.get("duracion_s", 1)
        
        if "velocidad_concentrica_m_s" in df.columns and "velocidad_eccentrica_m_s" in df.columns:
            df["ratio_excentrico_concentrico"] = df["velocidad_eccentrica_m_s"] / df["velocidad_concentrica_m_s"]
        
        # Procesamiento temporal profesional
        if "fecha" in df.columns:
            df["fecha"] = pd.to_datetime(df["fecha"], errors="coerce")
            df["dia_semana"] = df["fecha"].dt.day_name()
            df["semana_entrenamiento"] = df["fecha"].dt.isocalendar().week
        
        # M√©tricas de calidad
        if "repeticion" in df.columns:
            df["fatiga_intra_serie"] = df.groupby(["atleta", "ejercicio", "fecha"])["velocidad_concentrica_m_s"].transform(
                lambda x: (x.iloc[0] - x.iloc[-1]) / x.iloc[0] * 100 if len(x) > 1 else 0
            )
        
        # Limpieza cient√≠fica
        initial_rows = len(df)
        df.dropna(subset=["atleta", "ejercicio"], inplace=True)
        if "carga_kg" in df.columns:
            df = df[df["carga_kg"] > 0]  # Eliminar cargas inv√°lidas
        final_rows = len(df)
        
        log.info(f"Procesamiento Android completado: {initial_rows} -> {final_rows} filas v√°lidas")
        return df

    except Exception as e:
        log.error(f"Error en procesamiento Android: {str(e)}")
        return df

def preprocess_generic_data(df: pd.DataFrame) -> pd.DataFrame:
    """Procesamiento gen√©rico para formatos desconocidos"""
    # Conversi√≥n num√©rica autom√°tica
    for col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='ignore')
    
    return df

# ==============================
# AN√ÅLISIS CIENT√çFICO AVANZADO
# ==============================

def generate_comprehensive_stats(df: pd.DataFrame, file_type: str) -> str:
    """Genera estad√≠sticas cient√≠ficas completas seg√∫n el tipo de archivo"""
    stats_lines = []
    
    if file_type.startswith("encoder_horizontal"):
        stats_lines.extend(generate_horizontal_stats(df, file_type))
    elif file_type.startswith("encoder_vertical"):
        stats_lines.extend(generate_vertical_stats(df, file_type))
    else:
        stats_lines.extend(generate_generic_stats(df))
    
    return "\n".join(stats_lines)

def generate_horizontal_stats(df: pd.DataFrame, file_type: str) -> List[str]:
    """Estad√≠sticas para Encoder Horizontal"""
    stats = []
    stats.append("üèÉ AN√ÅLISIS ENCODER HORIZONTAL - DATOS DE SPRINT")
    stats.append("=" * 60)
    stats.append(f"‚Ä¢ Tipo de datos: {file_type.upper().replace('_', ' ')}")
    stats.append(f"‚Ä¢ Total de registros: {df.shape[0]:,}")
    
    if file_type == "velocidad":
        if "velocidad_m_s" in df.columns:
            stats.extend(analyze_velocity_data(df))
    elif file_type == "aceleracion":
        if "aceleracion_m_s2" in df.columns:
            stats.extend(analyze_acceleration_data(df))
    elif file_type == "distancia":
        if "distancia_m" in df.columns:
            stats.extend(analyze_distance_data(df))
    elif file_type == "velocidad_vs_distancia":
        if "velocidad_m_s" in df.columns and "distancia_m" in df.columns:
            stats.extend(analyze_velocity_distance_data(df))
    elif file_type == "resumen":
        stats.extend(analyze_summary_data(df))
    
    return stats

def generate_vertical_stats(df: pd.DataFrame, file_type: str) -> List[str]:
    """Estad√≠sticas para Encoder Vertical"""
    stats = []
    stats.append("üèãÔ∏è AN√ÅLISIS ENCODER VERTICAL - FUERZA Y POTENCIA")
    stats.append("=" * 60)
    stats.append(f"‚Ä¢ Tipo de datos: {file_type.upper().replace('_', ' ')}")
    stats.append(f"‚Ä¢ Total de registros: {df.shape[0]:,}")
    
    if "atleta" in df.columns:
        stats.append(f"‚Ä¢ Atletas √∫nicos: {df['atleta'].nunique()}")
    
    if "ejercicio" in df.columns:
        stats.append(f"‚Ä¢ Ejercicios √∫nicos: {df['ejercicio'].nunique()}")
    
    # M√©tricas espec√≠ficas de fuerza-velocidad
    if "carga_kg" in df.columns and "velocidad_media_m_s" in df.columns:
        stats.extend(analyze_force_velocity_data(df))
    
    if "velocidad_concentrica_m_s" in df.columns:
        stats.extend(analyze_velocity_metrics(df))
    
    return stats

def generate_generic_stats(df: pd.DataFrame) -> List[str]:
    """Estad√≠sticas gen√©ricas"""
    stats = []
    stats.append("üìä AN√ÅLISIS ESTAD√çSTICO GENERAL")
    stats.append("=" * 60)
    stats.append(f"‚Ä¢ Total de registros: {df.shape[0]:,}")
    stats.append(f"‚Ä¢ Total de variables: {df.shape[1]}")
    
    # An√°lisis de columnas num√©ricas
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) > 0:
        stats.append("\nüî¢ VARIABLES NUM√âRICAS:")
        for col in numeric_cols[:6]:  # Mostrar m√°ximo 6 columnas
            desc = df[col].describe()
            stats.append(f"‚Ä¢ {col}: Œº={desc['mean']:.2f} ¬± {desc['std']:.2f} | Range: {desc['min']:.2f}-{desc['max']:.2f}")
    
    return stats

def analyze_velocity_data(df: pd.DataFrame) -> List[str]:
    """An√°lisis espec√≠fico para datos de velocidad"""
    stats = []
    if "velocidad_m_s" in df.columns:
        v = df["velocidad_m_s"]
        stats.append(f"‚Ä¢ Velocidad m√°xima: {v.max():.2f} m/s ({v.max()*3.6:.1f} km/h)")
        stats.append(f"‚Ä¢ Velocidad promedio: {v.mean():.2f} m/s ({v.mean()*3.6:.1f} km/h)")
        stats.append(f"‚Ä¢ Aceleraci√≥n promedio: {(v.max() - v.iloc[0]) / len(v) * 10:.2f} m/s¬≤")
    return stats

def analyze_acceleration_data(df: pd.DataFrame) -> List[str]:
    """An√°lisis espec√≠fico para datos de aceleraci√≥n"""
    stats = []
    if "aceleracion_m_s2" in df.columns:
        a = df["aceleracion_m_s2"]
        stats.append(f"‚Ä¢ Aceleraci√≥n m√°xima: {a.max():.2f} m/s¬≤")
        stats.append(f"‚Ä¢ Aceleraci√≥n promedio: {a.mean():.2f} m/s¬≤")
        stats.append(f"‚Ä¢ Fuerza relativa estimada: {a.max() * 75:.0f} N (75kg)")
    return stats

def analyze_force_velocity_data(df: pd.DataFrame) -> List[str]:
    """An√°lisis de relaci√≥n fuerza-velocidad"""
    stats = []
    stats.append("\nüí™ PERFIL FUERZA-VELOCIDAD:")
    
    carga = df["carga_kg"]
    velocidad = df["velocidad_media_m_s"]
    
    stats.append(f"‚Ä¢ Carga m√°xima: {carga.max():.1f} kg")
    stats.append(f"‚Ä¢ Velocidad media: {velocidad.mean():.3f} m/s")
    
    if "potencia_w" in df.columns:
        potencia = df["potencia_w"]
        stats.append(f"‚Ä¢ Potencia pico: {potencia.max():.0f} W")
        stats.append(f"‚Ä¢ Potencia media: {potencia.mean():.0f} W")
    
    return stats

# ==============================
# IA PROFESIONAL - AN√ÅLISIS CIENT√çFICO
# ==============================

def run_professional_ai_analysis(df: pd.DataFrame, meta: dict, file_detection: dict) -> dict:
    """
    An√°lisis cient√≠fico profesional por IA especializada seg√∫n el tipo de datos
    """
    if not ai_client:
        return {
            "analysis": "üî¨ SERVICIO DE IA NO DISPONIBLE - Configure OPENAI_API_KEY",
            "python_code_for_charts": "",
            "charts_description": "",
            "recommendations": ["Configurar API key de OpenAI para an√°lisis profesional"]
        }
    
    try:
        # Estad√≠sticas espec√≠ficas seg√∫n el tipo de archivo
        comprehensive_stats = generate_comprehensive_stats(df, file_detection["file_type"])
        
        # Preparar datos completos para IA
        data_completa = df.to_csv(index=False)
        
        # Contexto cient√≠fico profesional espec√≠fico
        contexto = build_analysis_context(meta, file_detection, comprehensive_stats, df)
        
        # Prompt espec√≠fico seg√∫n el tipo de datos
        system_prompt = build_system_prompt(file_detection)
        
        user_prompt = f"{contexto}\n```csv\n{data_completa}\n```"

        log.info(f"üß† INICIANDO AN√ÅLISIS CIENT√çFICO PARA: {file_detection['file_type']}")
        
        response = ai_client.chat.completions.create(
            model=app.config["OPENAI_MODEL"],
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.1,
            max_tokens=8000,
            top_p=0.9,
            frequency_penalty=0.1,
            presence_penalty=0.1,
            response_format={"type": "json_object"}
        )
        
        log.info("‚úÖ AN√ÅLISIS CIENT√çFICO COMPLETADO")
        result = json.loads(response.choices[0].message.content)
        
        # Validaci√≥n de resultado
        if not all(key in result for key in ["analysis", "python_code_for_charts", "charts_description", "recommendations"]):
            raise ValueError("Respuesta de IA incompleta")
            
        return result
        
    except Exception as e:
        log.error(f"‚ùå ERROR EN AN√ÅLISIS CIENT√çFICO: {str(e)}")
        return {
            "analysis": f"‚ùå ERROR EN AN√ÅLISIS CIENT√çFICO: {str(e)}",
            "python_code_for_charts": "",
            "charts_description": "",
            "recommendations": ["Contactar soporte t√©cnico para an√°lisis cient√≠fico"]
        }

def build_analysis_context(meta: dict, file_detection: dict, stats: str, df: pd.DataFrame) -> str:
    """Construye el contexto espec√≠fico para el an√°lisis"""
    
    origin_descriptions = {
        "encoder_horizontal": "Encoder Horizontal - An√°lisis de Sprint y Aceleraci√≥n",
        "encoder_vertical_v1": "Encoder Vertical V1 (PyQt6) - Perfil Fuerza-Velocidad", 
        "app_android_encoder_vertical": "Encoder Vertical Android - Biomec√°nica Completa"
    }
    
    file_type_descriptions = {
        "velocidad": "Velocidad vs Tiempo en Sprint",
        "aceleracion": "Aceleraci√≥n vs Tiempo", 
        "distancia": "Distancia vs Tiempo",
        "velocidad_vs_distancia": "Velocidad vs Distancia",
        "resumen": "Resumen de M√©tricas de Sprint",
        "encoder_vertical_v1_completo": "Datos Completos Fuerza-Velocidad",
        "encoder_vertical_android_completo": "Biomec√°nica Completa de Levantamiento"
    }
    
    contexto = f"""
AN√ÅLISIS CIENT√çFICO PROFESIONAL - SISTEMA INERTIAX PRO
===================================================

INFORMACI√ìN DEL AN√ÅLISIS:
‚Ä¢ Entrenador: {meta.get('nombre_entrenador', 'Profesional del Deporte')}
‚Ä¢ Sistema de origen: {origin_descriptions.get(file_detection['origin_app'], file_detection['origin_app'])}
‚Ä¢ Tipo de datos: {file_type_descriptions.get(file_detection['file_type'], file_detection['file_type'])}
‚Ä¢ Confianza de detecci√≥n: {file_detection['confidence']:.1%}
‚Ä¢ Fecha de an√°lisis: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
‚Ä¢ Total de datos procesados: {df.shape[0]:,} registros

ESTAD√çSTICAS CIENT√çFICAS COMPLETAS:
{stats}

COLUMNAS DETECTADAS: {file_detection['columns_found']}

INSTRUCCIONES ESPEC√çFICAS PARA AN√ÅLISIS:
"""
    
    # Instrucciones espec√≠ficas por tipo de datos
    if file_detection["origin_app"] == "encoder_horizontal":
        contexto += """
AN√ÅLISIS DE SPRINT Y ACELERACI√ìN:
1. Evaluar perfil de aceleraci√≥n y velocidad m√°xima
2. Analizar t√©cnica de carrera y eficiencia mec√°nica  
3. Identificar puntos de fatiga y mantenimiento de velocidad
4. Recomendaciones para mejora de aceleraci√≥n y velocidad
"""
    elif file_detection["origin_app"] == "encoder_vertical_v1":
        contexto += """
AN√ÅLISIS DE FUERZA-VELOCIDAD:
1. Evaluar perfil individual fuerza-velocidad
2. Analizar relaci√≥n carga-velocidad con regresiones
3. Identificar zonas √≥ptimas de entrenamiento
4. Recomendaciones para desarrollo de fuerza y potencia
"""
    elif file_detection["origin_app"] == "app_android_encoder_vertical":
        contexto += """
AN√ÅLISIS BIOMEC√ÅNICO COMPLETO:
1. Evaluar t√©cnica de levantamiento y consistencia
2. Analizar fatiga intra-serie e inter-sesi√≥n
3. Identificar asimetr√≠as y desbalances
4. Optimizar prescripci√≥n de carga y volumen
"""

    contexto += "\nDATOS COMPLETOS PARA AN√ÅLISIS CIENT√çFICO:"
    return contexto

def build_system_prompt(file_detection: dict) -> str:
    """Construye el prompt del sistema seg√∫n el tipo de datos"""
    
    base_prompt = """
Eres un equipo de cient√≠ficos deportivos con PhD en Biomec√°nica, Fisiolog√≠a del Ejercicio y Anal√≠tica Deportiva. 
Tienes 20+ a√±os de experiencia en alto rendimiento y investigaci√≥n cient√≠fica.

PROTOCOLO DE AN√ÅLISIS CIENT√çFICO:
1. AN√ÅLISIS ESPEC√çFICO SEG√öN TIPO DE DATOS
2. METODOLOG√çA ESTAD√çSTICA RIGUROSA  
3. INTERPRETACI√ìN CIENT√çFICA BASADA EN EVIDENCIA
4. COMUNICACI√ìN PROFESIONAL Y ACCIONABLE

RESPONDER EN FORMATO JSON ESTRICTAMENTE:
{
    "analysis": "An√°lisis cient√≠fico completo...",
    "python_code_for_charts": "C√≥digo Python para gr√°ficos profesionales...",
    "charts_description": "Descripci√≥n detallada de visualizaciones...", 
    "recommendations": ["Recomendaci√≥n 1...", "Recomendaci√≥n 2..."]
}
"""
    
    # Especializaci√≥n seg√∫n tipo de datos
    if file_detection["origin_app"] == "encoder_horizontal":
        base_prompt += """
ENFOQUE PARA DATOS DE SPRINT:
- An√°lisis de curva de aceleraci√≥n y velocidad
- Identificaci√≥n de fases del sprint
- Eficiencia mec√°nica y t√©cnica de carrera
- M√©tricas de potencia y fatiga
"""
    elif file_detection["origin_app"] == "encoder_vertical":
        base_prompt += """
ENFOQUE PARA DATOS DE FUERZA:
- Perfiles individuales fuerza-velocidad
- An√°lisis de relaci√≥n carga-velocidad
- Eficiencia neuromuscular
- Prescripci√≥n de entrenamiento √≥ptimo
"""
    
    return base_prompt

# ==============================
# VISUALIZACIONES PROFESIONALES
# ==============================

def generate_professional_charts(df: pd.DataFrame, file_detection: dict) -> List[BytesIO]:
    """
    Genera visualizaciones cient√≠ficas profesionales seg√∫n el tipo de datos
    """
    charts = []
    plt.style.use('seaborn-v0_8-whitegrid')
    
    try:
        # Configuraci√≥n profesional
        professional_colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#3B1F2B']
        
        # Gr√°ficos espec√≠ficos seg√∫n el tipo de datos
        if file_detection["origin_app"] == "encoder_horizontal":
            charts.extend(generate_horizontal_charts(df, file_detection, professional_colors))
        elif "encoder_vertical" in file_detection["origin_app"]:
            charts.extend(generate_vertical_charts(df, file_detection, professional_colors))
        else:
            charts.extend(generate_generic_charts(df, professional_colors))
            
    except Exception as e:
        log.error(f"Error en generaci√≥n de gr√°ficos profesionales: {str(e)}")
    
    return charts

def generate_horizontal_charts(df: pd.DataFrame, file_detection: dict, colors: list) -> List[BytesIO]:
    """Gr√°ficos espec√≠ficos para Encoder Horizontal"""
    charts = []
    
    try:
        if file_detection["file_type"] == "velocidad" and "velocidad_m_s" in df.columns:
            # Gr√°fico de velocidad vs tiempo
            fig, ax = plt.subplots(figsize=(12, 6))
            
            if "tiempo_s" in df.columns:
                ax.plot(df["tiempo_s"], df["velocidad_m_s"], linewidth=2.5, color=colors[0])
                ax.set_xlabel('Tiempo (s)', fontsize=11, fontweight='bold')
            else:
                ax.plot(df.index, df["velocidad_m_s"], linewidth=2.5, color=colors[0])
                ax.set_xlabel('Muestras', fontsize=11, fontweight='bold')
            
            ax.set_ylabel('Velocidad (m/s)', fontsize=11, fontweight='bold')
            ax.set_title('PERFIL DE VELOCIDAD - AN√ÅLISIS DE SPRINT', fontsize=13, fontweight='bold')
            ax.grid(True, alpha=0.3)
            ax.set_facecolor('#f8f9fa')
            
            # A√±adir m√©tricas clave
            max_vel = df["velocidad_m_s"].max()
            avg_vel = df["velocidad_m_s"].mean()
            ax.axhline(y=max_vel, color='red', linestyle='--', alpha=0.7, label=f'M√°x: {max_vel:.2f} m/s')
            ax.axhline(y=avg_vel, color='green', linestyle='--', alpha=0.7, label=f'Prom: {avg_vel:.2f} m/s')
            ax.legend()
            
            plt.tight_layout()
            charts.append(save_plot_to_buffer(fig))
            
    except Exception as e:
        log.error(f"Error en gr√°ficos horizontales: {e}")
    
    return charts

def generate_vertical_charts(df: pd.DataFrame, file_detection: dict, colors: list) -> List[BytesIO]:
    """Gr√°ficos espec√≠ficos para Encoder Vertical"""
    charts = []
    
    try:
        # Gr√°fico de perfil fuerza-velocidad
        if "carga_kg" in df.columns and "velocidad_media_m_s" in df.columns:
            fig, ax = plt.subplots(figsize=(10, 6))
            
            if "atleta" in df.columns:
                for i, athlete in enumerate(df['atleta'].unique()[:4]):
                    athlete_data = df[df['atleta'] == athlete]
                    color = colors[i % len(colors)]
                    
                    scatter = ax.scatter(athlete_data['carga_kg'], athlete_data['velocidad_media_m_s'],
                                       c=color, alpha=0.7, s=60, label=athlete, edgecolors='white', linewidth=0.5)
                    
                    # L√≠nea de tendencia
                    if len(athlete_data) > 1:
                        z = np.polyfit(athlete_data['carga_kg'], athlete_data['velocidad_media_m_s'], 1)
                        p = np.poly1d(z)
                        x_range = np.linspace(athlete_data['carga_kg'].min(), athlete_data['carga_kg'].max(), 100)
                        ax.plot(x_range, p(x_range), color=color, linestyle='--', alpha=0.8, linewidth=2)
            else:
                ax.scatter(df['carga_kg'], df['velocidad_media_m_s'], alpha=0.7, s=50, color=colors[0])
            
            ax.set_xlabel('Carga (kg)', fontsize=11, fontweight='bold')
            ax.set_ylabel('Velocidad Media (m/s)', fontsize=11, fontweight='bold')
            ax.set_title('PERFIL FUERZA-VELOCIDAD', fontsize=13, fontweight='bold')
            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
            ax.grid(True, alpha=0.3)
            
            plt.tight_layout()
            charts.append(save_plot_to_buffer(fig))
            
    except Exception as e:
        log.error(f"Error en gr√°ficos verticales: {e}")
    
    return charts

def generate_generic_charts(df: pd.DataFrame, colors: list) -> List[BytesIO]:
    """Gr√°ficos gen√©ricos para datos desconocidos"""
    charts = []
    
    try:
        # Gr√°fico de distribuci√≥n de variables num√©ricas
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        
        if len(numeric_cols) > 0:
            fig, axes = plt.subplots(1, min(3, len(numeric_cols)), figsize=(15, 5))
            if len(numeric_cols) == 1:
                axes = [axes]
            
            for i, col in enumerate(numeric_cols[:3]):
                axes[i].hist(df[col].dropna(), bins=20, alpha=0.7, color=colors[i], edgecolor='black')
                axes[i].set_title(f'Distribuci√≥n de {col}', fontweight='bold')
                axes[i].set_xlabel(col)
                axes[i].set_ylabel('Frecuencia')
                axes[i].grid(True, alpha=0.3)
            
            plt.tight_layout()
            charts.append(save_plot_to_buffer(fig))
            
    except Exception as e:
        log.error(f"Error en gr√°ficos gen√©ricos: {e}")
    
    return charts

def save_plot_to_buffer(fig) -> BytesIO:
    """Guarda gr√°fico en buffer con calidad profesional"""
    buf = BytesIO()
    fig.savefig(buf, format='png', dpi=300, bbox_inches='tight', 
                facecolor='white', edgecolor='none')
    buf.seek(0)
    plt.close(fig)
    return buf

# ==============================
# GENERACI√ìN DE REPORTES PROFESIONALES
# ==============================

def generate_professional_pdf(ai_result: dict, charts: List[BytesIO], meta: dict, file_detection: dict) -> str:
    """Genera reporte PDF profesional"""
    try:
        pdf_path = os.path.join(app.config["UPLOAD_DIR"], f"reporte_profesional_{uuid.uuid4().hex}.pdf")
        
        doc = SimpleDocTemplate(pdf_path, pagesize=A4, topMargin=0.5*inch)
        styles = getSampleStyleSheet()
        
        # Estilos profesionales personalizados
        title_style = ParagraphStyle(
            'ProfessionalTitle',
            parent=styles['Heading1'],
            fontSize=16,
            textColor=colors.HexColor('#2E86AB'),
            spaceAfter=30,
            alignment=1
        )
        
        subtitle_style = ParagraphStyle(
            'ProfessionalSubtitle', 
            parent=styles['Heading2'],
            fontSize=12,
            textColor=colors.HexColor('#3B1F2B'),
            spaceAfter=12
        )
        
        story = []
        
        # Header profesional
        story.append(Paragraph("REPORTE CIENT√çFICO INERTIAX PRO", title_style))
        story.append(Spacer(1, 10))
        
        # Informaci√≥n del an√°lisis espec√≠fica
        origin_names = {
            "encoder_horizontal": "Encoder Horizontal - An√°lisis de Sprint",
            "encoder_vertical_v1": "Encoder Vertical V1 - Fuerza-Velocidad", 
            "app_android_encoder_vertical": "Encoder Vertical Android - Biomec√°nica"
        }
        
        info_text = f"""
        <b>Entrenador:</b> {meta.get('nombre_entrenador', 'Profesional')}<br/>
        <b>Sistema analizado:</b> {origin_names.get(file_detection['origin_app'], file_detection['origin_app'])}<br/>
        <b>Tipo de datos:</b> {file_detection['file_type'].replace('_', ' ').title()}<br/>
        <b>Fecha de generaci√≥n:</b> {datetime.now().strftime('%Y-%m-%d %H:%M')}<br/>
        <b>Confianza de detecci√≥n:</b> {file_detection['confidence']:.1%}
        """
        story.append(Paragraph(info_text, styles['Normal']))
        story.append(Spacer(1, 20))
        
        # An√°lisis cient√≠fico
        story.append(Paragraph("AN√ÅLISIS CIENT√çFICO PROFESIONAL", subtitle_style))
        analysis_text = ai_result.get('analysis', 'An√°lisis no disponible').replace('\n', '<br/>')
        story.append(Paragraph(analysis_text, styles['Normal']))
        story.append(Spacer(1, 20))
        
        # Gr√°ficos profesionales
        if charts:
            story.append(Paragraph("VISUALIZACIONES CIENT√çFICAS", subtitle_style))
            story.append(Spacer(1, 10))
            
            for i, chart in enumerate(charts[:4]):  # M√°ximo 4 gr√°ficos
                try:
                    chart.seek(0)
                    img = ReportLabImage(chart, width=6*inch, height=4*inch)
                    story.append(img)
                    story.append(Spacer(1, 5))
                    desc = ai_result.get('charts_description', 'Visualizaci√≥n cient√≠fica').split('.')[i] if i < len(ai_result.get('charts_description', '').split('.')) else 'Gr√°fico profesional'
                    story.append(Paragraph(f"Figura {i+1}: {desc}", styles['Italic']))
                    story.append(Spacer(1, 15))
                except Exception as e:
                    continue
        
        # Recomendaciones profesionales
        story.append(Paragraph("RECOMENDACIONES CIENT√çFICAS", subtitle_style))
        recommendations = ai_result.get('recommendations', [])
        if isinstance(recommendations, list):
            for rec in recommendations[:8]:  # M√°ximo 8 recomendaciones
                story.append(Paragraph(f"‚Ä¢ {rec}", styles['Normal']))
                story.append(Spacer(1, 5))
        else:
            story.append(Paragraph(str(recommendations), styles['Normal']))
        
        # Footer profesional
        story.append(Spacer(1, 20))
        footer_text = """
        <i>Reporte generado por InertiaX Professional Analysis System v3.0<br/>
        Sistema multi-formato para an√°lisis deportivo cient√≠fico<br/>
        ¬© 2024 InertiaX - Todos los derechos reservados</i>
        """
        story.append(Paragraph(footer_text, styles['Italic']))
        
        doc.build(story)
        return pdf_path
        
    except Exception as e:
        log.error(f"Error generando PDF profesional: {str(e)}")
        # Crear PDF de error profesional
        error_path = os.path.join(app.config["UPLOAD_DIR"], f"error_{uuid.uuid4().hex}.pdf")
        c = canvas.Canvas(error_path)
        c.drawString(100, 750, "INERTIAX PRO - ERROR EN REPORTE")
        c.drawString(100, 730, f"Error: {str(e)}")
        c.drawString(100, 710, "Contacte al soporte t√©cnico")
        c.save()
        return error_path

# ==============================
# RUTAS PROFESIONALES ACTUALIZADAS
# ==============================

@app.route("/")
def index():
    return render_template("index.html")

@app.route("/health")
def health():
    return jsonify({
        "status": "ok", 
        "message": "InertiaX Professional API Running",
        "version": "3.0.0",
        "timestamp": datetime.now().isoformat()
    })

@app.route("/upload", methods=["GET", "POST"])
def upload():
    if request.method == "GET":
        # Al refrescar, recuperar el job y mostrar previsualizaci√≥n
        job_id = session.get("job_id")
        if not job_id:
            return render_template("index.html")

        meta = _load_meta(job_id)
        if not meta:
            return render_template("index.html")

        try:
            df = parse_dataframe(meta["file_path"])
            file_detection = detect_file_type(df, meta["file_name"], meta["form"]["origen_app"])
            df = preprocess_data_by_origin(df, meta["form"]["origen_app"], file_detection["file_type"])

            table_html = df.head(15).to_html(
                classes="table table-striped table-bordered table-hover table-sm",
                index=False,
                escape=False
            )

            return render_template(
                "index.html",
                table_html=table_html,
                filename=meta["file_name"],
                form_data=meta["form"],
                file_detection=file_detection,
                mensaje=("üîì ACCESO PREMIUM ACTIVADO - An√°lisis profesional disponible"
                        if meta["payment_ok"]
                        else None),
                show_payment=(not meta["payment_ok"]),
            )
        except:
            return render_template("index.html")

    # ===== POST =====
    f = request.files.get("file")
    if not f or f.filename == "":
        return render_template("index.html", error="‚ùå ARCHIVO NO ESPECIFICADO - Seleccione un archivo para an√°lisis")

    try:
        job_id = _ensure_job()
        session.modified = True

        form = {
            "nombre_entrenador": request.form.get("nombre_entrenador", "").strip(),
            "origen_app": request.form.get("origen_app", "").strip(),
            "codigo_invitado": request.form.get("codigo_invitado", "").strip(),
        }

        log.info(f"üì• Solicitud de an√°lisis profesional de: {form['nombre_entrenador']}")

        code = form.get("codigo_invitado", "")
        payment_ok = False
        mensaje = None
        if code and code in app.config["GUEST_CODES"]:
            payment_ok = True
            mensaje = "üîì ACCESO PREMIUM ACTIVADO - An√°lisis profesional disponible"

        ext = os.path.splitext(f.filename)[1].lower()
        safe_name = f"{uuid.uuid4().hex}{ext}"
        save_path = os.path.join(_job_dir(job_id), safe_name)
        f.save(save_path)

        # Detecci√≥n autom√°tica del tipo de archivo
        df = parse_dataframe(save_path)
        file_detection = detect_file_type(df, f.filename, form["origen_app"])
        
        meta = {
            "file_name": f.filename,
            "file_path": save_path,
            "payment_ok": payment_ok,
            "form": form,
            "file_detection": file_detection,
            "upload_time": datetime.now().isoformat(),
        }
        _save_meta(job_id, meta)

        # Procesamiento espec√≠fico
        df = preprocess_data_by_origin(df, form["origen_app"], file_detection["file_type"])

        table_html = df.head(15).to_html(
            classes="table table-striped table-bordered table-hover table-sm",
            index=False,
            escape=False
        )

        return render_template(
            "index.html",
            table_html=table_html,
            filename=f.filename,
            form_data=form,
            file_detection=file_detection,
            mensaje=mensaje,
            show_payment=(not payment_ok),
        )

    except Exception as e:
        log.error(f"‚ùå Error en procesamiento: {str(e)}")
        return render_template("index.html", error=f"‚ùå ERROR EN PROCESAMIENTO: {str(e)}")

@app.route("/create_preference", methods=["POST"])
def create_preference():
    """Sistema de pago profesional"""
    if not mp:
        return jsonify(error="SISTEMA DE PAGO NO CONFIGURADO"), 500

    job_id = session.get("job_id")
    if not job_id:
        return jsonify(error="SESI√ìN INV√ÅLIDA"), 400

    # Precio profesional por servicio premium
    price = 990  # Servicio profesional premium

    pref_data = {
        "items": [{
            "title": "InertiaX Pro - An√°lisis Cient√≠fico Premium",
            "description": "An√°lisis biomec√°nico profesional con IA cient√≠fica multi-formato",
            "quantity": 1,
            "unit_price": price,
            "currency_id": "CLP",
        }],
        "back_urls": {
            "success": f"{app.config['DOMAIN_URL']}/success",
            "failure": f"{app.config['DOMAIN_URL']}/cancel", 
            "pending": f"{app.config['DOMAIN_URL']}/cancel",
        },
        "auto_return": "approved",
    }

    try:
        pref = mp.preference().create(pref_data)
        return jsonify(pref.get("response", {}))
    except Exception as e:
        log.error(f"Error en sistema de pago: {e}")
        return jsonify(error=str(e)), 500

@app.route("/success")
def success():
    """Pago exitoso - profesional"""
    job_id = session.get("job_id")
    if not job_id:
        return redirect(url_for("index"))

    meta = _load_meta(job_id)
    meta["payment_ok"] = True
    _save_meta(job_id, meta)
    
    log.info(f"‚úÖ Pago exitoso para job: {job_id}")
    return render_template("success.html")

@app.route("/cancel") 
def cancel():
    """Pago cancelado"""
    return render_template("cancel.html")

@app.route("/generate_report")
def generate_report():
    """Generaci√≥n de reporte profesional completo"""
    job_id = session.get("job_id")
    if not job_id:
        return redirect(url_for("index"))

    meta = _load_meta(job_id)
    if not meta.get("payment_ok"):
        return redirect(url_for("index"))

    file_path = meta.get("file_path")
    if not file_path or not os.path.exists(file_path):
        return render_template("index.html", error="‚ùå ARCHIVO NO ENCONTRADO")

    try:
        log.info("üöÄ INICIANDO GENERACI√ìN DE REPORTE PROFESIONAL MULTI-FORMATO")
        
        # 1. Carga y procesamiento profesional
        df = parse_dataframe(file_path)
        file_detection = meta.get("file_detection", {})
        df = preprocess_data_by_origin(df, meta.get("form", {}).get("origen_app", ""), file_detection.get("file_type", "desconocido"))
        
        log.info(f"üìä Dataset profesional cargado: {df.shape[0]} registros - Tipo: {file_detection.get('file_type', 'desconocido')}")

        # 2. An√°lisis cient√≠fico con IA espec√≠fico
        log.info("üß† EJECUTANDO AN√ÅLISIS CIENT√çFICO ESPEC√çFICO...")
        ai_result = run_professional_ai_analysis(df, meta.get("form", {}), file_detection)
        
        # 3. Generaci√≥n de gr√°ficos profesionales espec√≠ficos
        log.info("üìà GENERANDO VISUALIZACIONES ESPEC√çFICAS...")
        professional_charts = generate_professional_charts(df, file_detection)
        
        # 4. Gr√°ficos adicionales de IA si est√°n disponibles
        ai_charts = []
        python_code = ai_result.get("python_code_for_charts", "")
        if python_code:
            try:
                ai_charts = execute_ai_charts_code(python_code, df)
            except Exception as e:
                log.error(f"Error en gr√°ficos IA: {e}")

        # 5. Generaci√≥n de reporte PDF profesional espec√≠fico
        log.info("üìÑ GENERANDO REPORTE PDF PROFESIONAL...")
        all_charts = professional_charts + ai_charts
        pdf_path = generate_professional_pdf(ai_result, all_charts, meta.get("form", {}), file_detection)

        # 6. Creaci√≥n de paquete profesional
        zip_path = os.path.join(_job_dir(job_id), f"reporte_profesional_{uuid.uuid4().hex}.zip")
        with zipfile.ZipFile(zip_path, "w") as zf:
            zf.write(pdf_path, "INERTIAX_PRO_Reporte_Cientifico.pdf")
            zf.write(file_path, f"datos_originales/{os.path.basename(meta.get('file_name', 'datos.csv'))}")
            
            # Agregar datos procesados
            processed_data_path = os.path.join(_job_dir(job_id), "datos_procesados.csv")
            df.to_csv(processed_data_path, index=False)
            zf.write(processed_data_path, "datos_procesados/analisis_completo.csv")

        # Limpieza profesional
        try:
            os.remove(pdf_path)
            os.remove(processed_data_path)
        except:
            pass

        log.info("‚úÖ REPORTE PROFESIONAL MULTI-FORMATO GENERADO EXITOSAMENTE")
            
        return send_file(
            zip_path, 
            as_attachment=True, 
            download_name=f"InertiaX_Pro_Reporte_{datetime.now().strftime('%Y%m%d_%H%M')}.zip"
        )
        
    except Exception as e:
        log.error(f"‚ùå ERROR EN GENERACI√ìN DE REPORTE: {str(e)}")
        return render_template("index.html", error=f"‚ùå ERROR CR√çTICO: {str(e)}")

def execute_ai_charts_code(python_code: str, df: pd.DataFrame) -> List[BytesIO]:
    """Ejecuta c√≥digo Python de gr√°ficos generado por IA"""
    if not python_code.strip():
        return []
    
    try:
        local_vars = {
            'df': df, 
            'BytesIO': BytesIO, 
            'plt': plt,
            'sns': sns,
            'np': np,
            'pd': pd,
            'charts': []
        }
        
        exec(python_code, local_vars)
        
        charts = local_vars.get('charts', [])
        if not isinstance(charts, list):
            charts = []
            
        return charts
        
    except Exception as e:
        log.error(f"Error ejecutando c√≥digo de gr√°ficos IA: {e}")
        return []

@app.route("/preview_analysis")
def preview_analysis():
    """Vista previa profesional del an√°lisis"""
    job_id = session.get("job_id")
    if not job_id:
        return redirect(url_for("index"))

    meta = _load_meta(job_id)
    if not meta.get("payment_ok"):
        return redirect(url_for("index"))

    file_path = meta.get("file_path")
    if not file_path or not os.path.exists(file_path):
        return render_template("index.html", error="‚ùå ARCHIVO NO ENCONTRADO")

    try:
        df = parse_dataframe(file_path)
        file_detection = meta.get("file_detection", {})
        df = preprocess_data_by_origin(df, meta.get("form", {}).get("origen_app", ""), file_detection.get("file_type", "desconocido"))
        
        # An√°lisis cient√≠fico espec√≠fico
        ai_result = run_professional_ai_analysis(df, meta.get("form", {}), file_detection)
        
        return render_template(
            "preview.html",
            ai_analysis=ai_result.get("analysis", "An√°lisis cient√≠fico en progreso..."),
            recommendations=ai_result.get("recommendations", []),
            filename=meta.get("file_name"),
            file_detection=file_detection
        )
        
    except Exception as e:
        log.error(f"Error en vista previa: {e}")
        return render_template("index.html", error=f"Error en vista previa: {e}")

# ==============================
# MANEJO DE ERRORES PROFESIONAL
# ==============================

@app.errorhandler(413)
def too_large(_e):
    return render_template("index.html", error="‚ùå ARCHIVO DEMASIADO GRANDE - M√°ximo 50MB")

@app.errorhandler(404)
def not_found(_e):
    return render_template("index.html", error="‚ùå RECURSO NO ENCONTRADO")

@app.errorhandler(500)
def internal_error(_e):
    return render_template("index.html", error="‚ùå ERROR INTERNO DEL SERVIDOR")

@app.errorhandler(Exception)
def global_error(e):
    log.exception("Error no controlado en sistema profesional")
    return render_template("index.html", error=f"‚ùå ERROR DEL SISTEMA: {str(e)}")

# ==============================
# INICIALIZACI√ìN PROFESIONAL
# ==============================

if __name__ == "__main__":
    port = int(os.environ.get("PORT", "10000"))
    log.info(f"üöÄ INERTIAX PROFESSIONAL v3.0 STARTING ON PORT {port}")
    log.info("‚úÖ SISTEMA MULTI-FORMATO ACTIVADO: Encoder Horizontal, Encoder Vertical V1, Encoder Vertical Android")
    app.run(host="0.0.0.0", port=port, debug=False)
